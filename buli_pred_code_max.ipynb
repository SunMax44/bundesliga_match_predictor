{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca3b2bb6-5cd0-4929-a969-d8d755b2b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2089bf0-26cd-4845-9407-dafb1652278c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final combined DataFrame with parsed dates:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/72/b7zxktp96cz1n4tjk3mlbj5w0000gn/T/ipykernel_10198/1889391515.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  buli_df['Date'] = pd.to_datetime(buli_df['Date'], dayfirst=True, errors='coerce')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTHG</th>\n",
       "      <th>HTAG</th>\n",
       "      <th>HTR</th>\n",
       "      <th>...</th>\n",
       "      <th>B365CAHA</th>\n",
       "      <th>PCAHH</th>\n",
       "      <th>PCAHA</th>\n",
       "      <th>MaxCAHH</th>\n",
       "      <th>MaxCAHA</th>\n",
       "      <th>AvgCAHH</th>\n",
       "      <th>AvgCAHA</th>\n",
       "      <th>Unnamed: 70</th>\n",
       "      <th>Unnamed: 71</th>\n",
       "      <th>Unnamed: 72</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>2015-08-14</td>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D1</td>\n",
       "      <td>2015-08-15</td>\n",
       "      <td>Augsburg</td>\n",
       "      <td>Hertha</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1</td>\n",
       "      <td>2015-08-15</td>\n",
       "      <td>Darmstadt</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D1</td>\n",
       "      <td>2015-08-15</td>\n",
       "      <td>Dortmund</td>\n",
       "      <td>M'gladbach</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D1</td>\n",
       "      <td>2015-08-15</td>\n",
       "      <td>Leverkusen</td>\n",
       "      <td>Hoffenheim</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5503</th>\n",
       "      <td>D1</td>\n",
       "      <td>2013-05-18</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Leverkusen</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504</th>\n",
       "      <td>D1</td>\n",
       "      <td>2013-05-18</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>Fortuna Dusseldorf</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5505</th>\n",
       "      <td>D1</td>\n",
       "      <td>2013-05-18</td>\n",
       "      <td>M'gladbach</td>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5506</th>\n",
       "      <td>D1</td>\n",
       "      <td>2013-05-18</td>\n",
       "      <td>Nurnberg</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5507</th>\n",
       "      <td>D1</td>\n",
       "      <td>2013-05-18</td>\n",
       "      <td>Stuttgart</td>\n",
       "      <td>Mainz</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5508 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Div       Date       HomeTeam            AwayTeam  FTHG  FTAG FTR  HTHG  \\\n",
       "0     D1 2015-08-14  Bayern Munich             Hamburg     5     0   H     1   \n",
       "1     D1 2015-08-15       Augsburg              Hertha     0     1   A     0   \n",
       "2     D1 2015-08-15      Darmstadt            Hannover     2     2   D     1   \n",
       "3     D1 2015-08-15       Dortmund          M'gladbach     4     0   H     3   \n",
       "4     D1 2015-08-15     Leverkusen          Hoffenheim     2     1   H     1   \n",
       "...   ..        ...            ...                 ...   ...   ...  ..   ...   \n",
       "5503  D1 2013-05-18        Hamburg          Leverkusen     0     1   A     0   \n",
       "5504  D1 2013-05-18       Hannover  Fortuna Dusseldorf     3     0   H     1   \n",
       "5505  D1 2013-05-18     M'gladbach       Bayern Munich     3     4   A     3   \n",
       "5506  D1 2013-05-18       Nurnberg       Werder Bremen     3     2   H     0   \n",
       "5507  D1 2013-05-18      Stuttgart               Mainz     2     2   D     2   \n",
       "\n",
       "      HTAG HTR  ...  B365CAHA  PCAHH  PCAHA  MaxCAHH  MaxCAHA  AvgCAHH  \\\n",
       "0        0   H  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "1        0   D  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "2        0   H  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "3        0   H  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "4        1   D  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "...    ...  ..  ...       ...    ...    ...      ...      ...      ...   \n",
       "5503     0   D  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "5504     0   H  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "5505     2   H  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "5506     1   A  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "5507     2   D  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "\n",
       "      AvgCAHA  Unnamed: 70  Unnamed: 71  Unnamed: 72  \n",
       "0         NaN          NaN          NaN          NaN  \n",
       "1         NaN          NaN          NaN          NaN  \n",
       "2         NaN          NaN          NaN          NaN  \n",
       "3         NaN          NaN          NaN          NaN  \n",
       "4         NaN          NaN          NaN          NaN  \n",
       "...       ...          ...          ...          ...  \n",
       "5503      NaN          NaN          NaN          NaN  \n",
       "5504      NaN          NaN          NaN          NaN  \n",
       "5505      NaN          NaN          NaN          NaN  \n",
       "5506      NaN          NaN          NaN          NaN  \n",
       "5507      NaN          NaN          NaN          NaN  \n",
       "\n",
       "[5508 rows x 141 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the folder path where CSV files are stored\n",
    "folder_path = 'data/'\n",
    "\n",
    "# Use glob to find all CSV files in the specified folder\n",
    "all_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# Use a list comprehension to read each CSV file into a DataFrame and ensure 'Date' is string\n",
    "df_list = []\n",
    "for file in all_files:\n",
    "    try:\n",
    "        # Read each CSV and convert 'Date' to string format\n",
    "        buli_df = pd.read_csv(file, encoding='ISO-8859-1', dtype={'Date': str})\n",
    "        df_list.append(buli_df)\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"ParserError parsing {file}: {e}\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"UnicodeDecodeError in {file}: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "buli_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Standardize and parse the 'Date' column\n",
    "buli_df['Date'] = buli_df['Date'].str.strip()  # Remove extra whitespace\n",
    "buli_df['Date'] = buli_df['Date'].replace(r'[/-]', '-', regex=True)  # Replace separators with '-'\n",
    "\n",
    "# Attempt to parse dates as `dayfirst` and handle both `dd/mm/yyyy` and `dd/mm/yy`\n",
    "buli_df['Date'] = buli_df['Date'].apply(lambda x: re.sub(r'(\\d{2}/\\d{2}/)(\\d{2})$', r'\\120\\2', x))\n",
    "buli_df['Date'] = pd.to_datetime(buli_df['Date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# Check for any remaining NaT values in 'Date' after parsing\n",
    "missing_dates = buli_df[buli_df['Date'].isna()]\n",
    "if not missing_dates.empty:\n",
    "    print(\"Warning: Some dates could not be parsed after concatenation.\")\n",
    "    print(missing_dates)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Final combined DataFrame with parsed dates:\")\n",
    "buli_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c54c0ca8-27e2-426e-a160-6ffd4ce0815e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTHG</th>\n",
       "      <th>HTAG</th>\n",
       "      <th>HTR</th>\n",
       "      <th>...</th>\n",
       "      <th>B365CAHA</th>\n",
       "      <th>PCAHH</th>\n",
       "      <th>PCAHA</th>\n",
       "      <th>MaxCAHH</th>\n",
       "      <th>MaxCAHA</th>\n",
       "      <th>AvgCAHH</th>\n",
       "      <th>AvgCAHA</th>\n",
       "      <th>Unnamed: 70</th>\n",
       "      <th>Unnamed: 71</th>\n",
       "      <th>Unnamed: 72</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>2006-08-11</td>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>Dortmund</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D1</td>\n",
       "      <td>2006-08-12</td>\n",
       "      <td>Leverkusen</td>\n",
       "      <td>Aachen</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1</td>\n",
       "      <td>2006-08-12</td>\n",
       "      <td>Mainz</td>\n",
       "      <td>Bochum</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D1</td>\n",
       "      <td>2006-08-12</td>\n",
       "      <td>M'gladbach</td>\n",
       "      <td>Cottbus</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D1</td>\n",
       "      <td>2006-08-12</td>\n",
       "      <td>Schalke 04</td>\n",
       "      <td>Ein Frankfurt</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5503</th>\n",
       "      <td>D1</td>\n",
       "      <td>2024-05-18</td>\n",
       "      <td>Heidenheim</td>\n",
       "      <td>FC Koln</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.86</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.84</td>\n",
       "      <td>2.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504</th>\n",
       "      <td>D1</td>\n",
       "      <td>2024-05-18</td>\n",
       "      <td>Ein Frankfurt</td>\n",
       "      <td>RB Leipzig</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.93</td>\n",
       "      <td>1.96</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5505</th>\n",
       "      <td>D1</td>\n",
       "      <td>2024-05-18</td>\n",
       "      <td>Dortmund</td>\n",
       "      <td>Darmstadt</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5506</th>\n",
       "      <td>D1</td>\n",
       "      <td>2024-05-18</td>\n",
       "      <td>Hoffenheim</td>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5507</th>\n",
       "      <td>D1</td>\n",
       "      <td>2024-05-18</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>Bochum</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5508 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Div       Date       HomeTeam       AwayTeam  FTHG  FTAG FTR  HTHG  HTAG  \\\n",
       "0     D1 2006-08-11  Bayern Munich       Dortmund     2     0   H     1     0   \n",
       "1     D1 2006-08-12     Leverkusen         Aachen     3     0   H     2     0   \n",
       "2     D1 2006-08-12          Mainz         Bochum     2     1   H     1     0   \n",
       "3     D1 2006-08-12     M'gladbach        Cottbus     2     0   H     0     0   \n",
       "4     D1 2006-08-12     Schalke 04  Ein Frankfurt     1     1   D     1     0   \n",
       "...   ..        ...            ...            ...   ...   ...  ..   ...   ...   \n",
       "5503  D1 2024-05-18     Heidenheim        FC Koln     4     1   H     3     0   \n",
       "5504  D1 2024-05-18  Ein Frankfurt     RB Leipzig     2     2   D     0     1   \n",
       "5505  D1 2024-05-18       Dortmund      Darmstadt     4     0   H     2     0   \n",
       "5506  D1 2024-05-18     Hoffenheim  Bayern Munich     4     2   H     1     2   \n",
       "5507  D1 2024-05-18  Werder Bremen         Bochum     4     1   H     1     0   \n",
       "\n",
       "     HTR  ...  B365CAHA  PCAHH  PCAHA  MaxCAHH  MaxCAHA  AvgCAHH  AvgCAHA  \\\n",
       "0      H  ...       NaN    NaN    NaN      NaN      NaN      NaN      NaN   \n",
       "1      H  ...       NaN    NaN    NaN      NaN      NaN      NaN      NaN   \n",
       "2      H  ...       NaN    NaN    NaN      NaN      NaN      NaN      NaN   \n",
       "3      D  ...       NaN    NaN    NaN      NaN      NaN      NaN      NaN   \n",
       "4      H  ...       NaN    NaN    NaN      NaN      NaN      NaN      NaN   \n",
       "...   ..  ...       ...    ...    ...      ...      ...      ...      ...   \n",
       "5503   H  ...      2.08   1.86   2.07     1.95     2.09     1.84     2.01   \n",
       "5504   A  ...      2.00   1.93   1.96     2.40     2.00     2.05     1.84   \n",
       "5505   H  ...      2.00   1.87   2.02     1.88     2.11     1.85     2.01   \n",
       "5506   A  ...      1.90   2.01   1.90     2.04     1.94     1.95     1.90   \n",
       "5507   H  ...      2.01   1.90   2.00     1.93     2.04     1.86     1.99   \n",
       "\n",
       "      Unnamed: 70  Unnamed: 71  Unnamed: 72  \n",
       "0             NaN          NaN          NaN  \n",
       "1             NaN          NaN          NaN  \n",
       "2             NaN          NaN          NaN  \n",
       "3             NaN          NaN          NaN  \n",
       "4             NaN          NaN          NaN  \n",
       "...           ...          ...          ...  \n",
       "5503          NaN          NaN          NaN  \n",
       "5504          NaN          NaN          NaN  \n",
       "5505          NaN          NaN          NaN  \n",
       "5506          NaN          NaN          NaN  \n",
       "5507          NaN          NaN          NaN  \n",
       "\n",
       "[5508 rows x 141 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#formatting the date column to datetime format and sorting by date\n",
    "#buli_df['Date'] = pd.to_datetime(buli_df['Date'])\n",
    "\n",
    "buli_df = buli_df.sort_values(['Date']).reset_index(drop=True)\n",
    "buli_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3441895d-8e09-49f9-9796-1fc0f2f4d09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Div               0\n",
       "Date              0\n",
       "HomeTeam          0\n",
       "AwayTeam          0\n",
       "FTHG              0\n",
       "               ... \n",
       "AvgCAHH        3978\n",
       "AvgCAHA        3978\n",
       "Unnamed: 70    5508\n",
       "Unnamed: 71    5508\n",
       "Unnamed: 72    5508\n",
       "Length: 141, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for null values\n",
    "buli_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0afed34d-9f79-4edc-817d-c5bd06c15893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping rows & columns with all null values\n",
    "buli_df.dropna(axis=1, how='all', inplace=True) #dropped 3 columns\n",
    "buli_df.dropna(axis=0, how='all',inplace=True) #0 rows dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11a34f86-102c-4261-b4ce-3a960a0ca228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Div            0\n",
       "Date           0\n",
       "HomeTeam       0\n",
       "AwayTeam       0\n",
       "FTHG           0\n",
       "            ... \n",
       "PCAHA       3978\n",
       "MaxCAHH     3978\n",
       "MaxCAHA     3978\n",
       "AvgCAHH     3978\n",
       "AvgCAHA     3978\n",
       "Length: 138, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buli_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9588dee0-a2f4-4c19-9b1b-f754829408c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame with selected initial columns and past 7 games stats:\n"
     ]
    }
   ],
   "source": [
    "#code for including all games (first games of the season take the last games of the last season as past games)\n",
    "#5 past games\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "buli_df_red = buli_df[['Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']]\n",
    "\n",
    "df = buli_df_red\n",
    "\n",
    "# Define stats dictionary with the specified columns\n",
    "stats = {\n",
    "    'goals': {'scored': ('FTHG', 'FTAG'), 'conceded': ('FTAG', 'FTHG')},\n",
    "    'shots': {'taken': ('HS', 'AS'), 'conceded': ('AS', 'HS')},\n",
    "    'shots_on_target': {'taken': ('HST', 'AST'), 'conceded': ('AST', 'HST')},\n",
    "    'fouls': {'fouls': ('HF', 'AF'), 'fouled': ('AF', 'HF')},\n",
    "    'corners': {'taken': ('HC', 'AC'), 'conceded': ('AC', 'HC')},\n",
    "    'yellow_cards': {'received': ('HY', 'AY'), 'provoked': ('AY', 'HY')},\n",
    "    'red_cards': {'received': ('HR', 'AR'), 'provoked': ('AR', 'HR')},\n",
    "}\n",
    "\n",
    "# Define `npm` for the number of past matches to consider\n",
    "npm = 5\n",
    "\n",
    "# Initialize an empty list to accumulate each row's data as a dictionary\n",
    "rows_list = []\n",
    "\n",
    "# Iterate through each row to calculate rolling stats based on home and away perspectives\n",
    "for index, row in df.iterrows():\n",
    "    team_h = row['HomeTeam']\n",
    "    team_a = row['AwayTeam']\n",
    "    date = row['Date']\n",
    "    \n",
    "    # Get the past `npm` games for the home team, filtered by games before the current match date\n",
    "    past_matches_home = df[((df['HomeTeam'] == team_h) | (df['AwayTeam'] == team_h)) & (df['Date'] < date)]\n",
    "    past_matches_home = past_matches_home.tail(npm)\n",
    "\n",
    "    # Get the past `npm` games for the away team, filtered by games before the current match date\n",
    "    past_matches_away = df[((df['HomeTeam'] == team_a) | (df['AwayTeam'] == team_a)) & (df['Date'] < date)]\n",
    "    past_matches_away = past_matches_away.tail(npm)\n",
    "\n",
    "    # Initialize a dictionary to store the calculated stats for each row\n",
    "    row_stats = {\n",
    "        'Date': date,\n",
    "        'HomeTeam': team_h,\n",
    "        'AwayTeam': team_a,\n",
    "        'FTR': row['FTR'],\n",
    "        'FTHG': row['FTHG'],\n",
    "        'FTAG': row['FTAG'],\n",
    "    }\n",
    "    \n",
    "    # Calculate stats for the home team based on whether they played home or away in past matches\n",
    "    for stat, subcategories in stats.items():\n",
    "        for subcategory, columns in subcategories.items():\n",
    "            home_column, away_column = columns\n",
    "            # Sum the stat when the home team was actually playing at home\n",
    "            stat_home_as_home = past_matches_home.loc[past_matches_home['HomeTeam'] == team_h, home_column].sum()\n",
    "            # Sum the stat when the home team was actually playing as the away team\n",
    "            stat_home_as_away = past_matches_home.loc[past_matches_home['AwayTeam'] == team_h, away_column].sum()\n",
    "            row_stats[f'p_home_{stat}_{subcategory}'] = stat_home_as_home + stat_home_as_away\n",
    "            \n",
    "    # Calculate stats for the away team based on whether they played home or away in past matches\n",
    "    for stat, subcategories in stats.items():\n",
    "        for subcategory, columns in subcategories.items():\n",
    "            home_column, away_column = columns\n",
    "            # Sum the stat when the away team was actually playing at home\n",
    "            stat_away_as_home = past_matches_away.loc[past_matches_away['HomeTeam'] == team_a, home_column].sum()\n",
    "            # Sum the stat when the away team was actually playing as the away team\n",
    "            stat_away_as_away = past_matches_away.loc[past_matches_away['AwayTeam'] == team_a, away_column].sum()\n",
    "            row_stats[f'p_away_{stat}_{subcategory}'] = stat_away_as_home + stat_away_as_away\n",
    "\n",
    "    # Calculate points for the home team in the past `npm` games\n",
    "    points_home = (\n",
    "        (past_matches_home.loc[past_matches_home['HomeTeam'] == team_h, 'FTR'] == 'H').sum() * 3 +\n",
    "        (past_matches_home.loc[past_matches_home['AwayTeam'] == team_h, 'FTR'] == 'A').sum() * 3 +\n",
    "        (past_matches_home['FTR'] == 'D').sum() * 1\n",
    "    )\n",
    "    row_stats[f'p_home_points'] = points_home\n",
    "    \n",
    "    # Calculate points for the away team in the past `npm` games\n",
    "    points_away = (\n",
    "        (past_matches_away.loc[past_matches_away['HomeTeam'] == team_a, 'FTR'] == 'H').sum() * 3 +\n",
    "        (past_matches_away.loc[past_matches_away['AwayTeam'] == team_a, 'FTR'] == 'A').sum() * 3 +\n",
    "        (past_matches_away['FTR'] == 'D').sum() * 1\n",
    "    )\n",
    "    row_stats[f'p_away_points'] = points_away\n",
    "    \n",
    "    # Append the dictionary for this row to the list\n",
    "    rows_list.append(row_stats)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "rolling_stats = pd.DataFrame(rows_list)\n",
    "\n",
    "# Display the final DataFrame with only the desired columns\n",
    "print(\"Final DataFrame with selected initial columns and past 7 games stats:\")\n",
    "rolling_stats\n",
    "\n",
    "rolling_stats[\"FTR_num\"] = rolling_stats[\"FTR\"].apply(lambda x: 1 if x == \"D\" else (2 if x == \"H\" else 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b155bfc0-ac0d-4c62-9356-287f421dde7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5068990559186638\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.36      0.01      0.02       360\n",
      "           2       0.54      0.81      0.64       635\n",
      "           3       0.44      0.47      0.46       382\n",
      "\n",
      "    accuracy                           0.51      1377\n",
      "   macro avg       0.45      0.43      0.37      1377\n",
      "weighted avg       0.46      0.51      0.43      1377\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  4 248 108]\n",
      " [  3 514 118]\n",
      " [  4 198 180]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features_cl = rolling_stats.select_dtypes(\"number\").drop(columns=[\"FTHG\", \"FTAG\", \"FTR_num\"])\n",
    "target_cl = rolling_stats[\"FTR_num\"]\n",
    "X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(features_cl, target_cl, random_state = 0)\n",
    "\n",
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_cl)\n",
    "X_train_cl_scaled = scaler.transform(X_train_cl)\n",
    "X_test_cl_scaled = scaler.transform(X_test_cl)\n",
    "\n",
    "## logistic regression testing the categorical ftr\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100)\n",
    "log_reg.fit(X_train_cl_scaled, y_train_cl)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = log_reg.predict(X_test_cl_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_cl, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Displaying classification report and confusion matrix for deeper insights\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_cl, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_cl, y_pred))\n",
    "\n",
    "# Save the scaler to also scale the data that will be processed through the streamlit input\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "# save as LogRegr pickle file for streamlit\n",
    "with open('log_reg_model.pkl', 'wb') as file:\n",
    "    pickle.dump(log_reg, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e1c82f56-a04b-4952-a0ef-612ab052ddbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4989106753812636"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## KNN classifier for categorical FTR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features_cl_dt = rolling_stats.select_dtypes(\"number\").drop(columns=[\"FTHG\", \"FTAG\", \"FTR_num\"])\n",
    "target_cl = rolling_stats[\"FTR_num\"]\n",
    "X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(features_cl_dt, target_cl, random_state = 0)\n",
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_cl)\n",
    "X_train_cl_scaled = scaler.transform(X_train_cl)\n",
    "X_test_cl_scaled = scaler.transform(X_test_cl)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 140)\n",
    "knn.fit(X_train_cl_scaled, y_train_cl)\n",
    "knn.score(X_test_cl_scaled, y_test_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "fb04b911-237a-496b-afb8-a3b12aaf1fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.46114742193173563\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       360\n",
      "           2       0.46      1.00      0.63       635\n",
      "           3       0.00      0.00      0.00       382\n",
      "\n",
      "    accuracy                           0.46      1377\n",
      "   macro avg       0.15      0.33      0.21      1377\n",
      "weighted avg       0.21      0.46      0.29      1377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "## Decision Tree CL\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features_cl_dt = rolling_stats.select_dtypes(\"number\").drop(columns=[\"FTHG\", \"FTAG\", \"FTR_num\"])\n",
    "target_cl = rolling_stats[\"FTR_num\"]\n",
    "X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(features_cl_dt, target_cl, random_state = 0)\n",
    "\n",
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_cl)\n",
    "X_train_cl_scaled = scaler.transform(X_train_cl)\n",
    "X_test_cl_scaled = scaler.transform(X_test_cl)\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "tree.fit(X_train_cl_scaled, y_train_cl)\n",
    "\n",
    "pred = tree.predict(X_test_cl)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_cl, pred))\n",
    "print(classification_report(y_test_cl, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ed9a8-8438-43fa-96e7-61e57ebf6128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c63a680-7e25-408f-8145-91e60c131d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d53e68d2-a109-4229-b5a0-1f3815d39022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 750}\n",
      "Best Model Accuracy: 0.47639796659404504\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.39      0.05      0.09       339\n",
      "           2       0.48      0.86      0.62       605\n",
      "           3       0.46      0.28      0.35       433\n",
      "\n",
      "    accuracy                           0.48      1377\n",
      "   macro avg       0.44      0.40      0.35      1377\n",
      "weighted avg       0.45      0.48      0.40      1377\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 17 261  61]\n",
      " [  8 519  78]\n",
      " [ 19 294 120]]\n",
      "\n",
      "Feature Importances:\n",
      "                         Feature  Importance\n",
      "17  p_away_shots_conceded_last_6    0.160429\n",
      "2      p_home_shots_taken_last_6    0.157097\n",
      "3   p_home_shots_conceded_last_6    0.154840\n",
      "16     p_away_shots_taken_last_6    0.153349\n",
      "20     p_away_fouls_fouls_last_6    0.130598\n",
      "6      p_home_fouls_fouls_last_6    0.126598\n",
      "21    p_away_fouls_fouled_last_6    0.117089\n"
     ]
    }
   ],
   "source": [
    "# with Gridsearch cv\n",
    "# takes very very long time\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define features and target\n",
    "features_cl = rolling_stats.select_dtypes(\"number\").drop(columns=[\"FTHG\", \"FTAG\", \"FTR_num\"])\n",
    "target_cl = rolling_stats[\"FTR_num\"]\n",
    "\n",
    "# Step 2: Train a preliminary model to get feature importances\n",
    "preliminary_rf = RandomForestClassifier(random_state=42)\n",
    "preliminary_rf.fit(features_cl, target_cl)\n",
    "\n",
    "# Step 3: Filter features based on importance threshold\n",
    "importance_threshold = 0.04\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': features_cl.columns,\n",
    "    'Importance': preliminary_rf.feature_importances_\n",
    "})\n",
    "important_features = feature_importances[feature_importances['Importance'] > importance_threshold]['Feature']\n",
    "features_important = features_cl[important_features]\n",
    "\n",
    "# Step 4: Remove zero-variance columns from filtered features\n",
    "features_important = features_important.loc[:, features_important.var() > 0]\n",
    "\n",
    "# Step 5: Split and scale the filtered dataset\n",
    "X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(features_important, target_cl, random_state=42)\n",
    "\n",
    "# Scaling based on filtered features only\n",
    "scaler = MinMaxScaler()\n",
    "X_train_cl_scaled = scaler.fit_transform(X_train_cl)\n",
    "X_test_cl_scaled = scaler.transform(X_test_cl)\n",
    "\n",
    "# Step 6: Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 750, 1000, 1250],  # Include values around your optimal 1000\n",
    "    'max_depth': [None, 10, 20, 30],         # Standard depth settings\n",
    "    'min_samples_split': [2, 5, 10],         # Standard split settings\n",
    "    'min_samples_leaf': [1, 2, 4]   \n",
    "}\n",
    "\n",
    "# Initialize RandomForest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Step 7: Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Step 8: Fit GridSearchCV on the training data\n",
    "grid_search.fit(X_train_cl_scaled, y_train_cl)\n",
    "\n",
    "# Step 9: Get the best model from GridSearchCV\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Step 10: Evaluate the best model on the test data\n",
    "y_pred = best_rf_model.predict(X_test_cl_scaled)\n",
    "accuracy = accuracy_score(y_test_cl, y_pred)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"Best Model Accuracy: {accuracy}\")\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_cl, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_cl, y_pred))\n",
    "\n",
    "# Feature importance analysis for the final tuned model\n",
    "final_feature_importances = pd.DataFrame({\n",
    "    'Feature': important_features,  # Now using the selected important features only\n",
    "    'Importance': best_rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(final_feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "cbc1e933-7224-49b3-82cd-216ff2189491",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[222], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Scaling based on filtered features only\u001b[39;00m\n\u001b[1;32m     34\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[0;32m---> 35\u001b[0m X_train_cl_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train_cl)\n\u001b[1;32m     36\u001b[0m X_test_cl_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test_cl)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Define parameter distribution for RandomizedSearchCV\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:450\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:490\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    487\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m    489\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 490\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    491\u001b[0m     X,\n\u001b[1;32m    492\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_pass,\n\u001b[1;32m    493\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_array_api\u001b[38;5;241m.\u001b[39msupported_float_dtypes(xp),\n\u001b[1;32m    494\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    495\u001b[0m )\n\u001b[1;32m    497\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    498\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    875\u001b[0m pandas_requires_conversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    876\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[1;32m    877\u001b[0m )\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[0;32m--> 879\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(\u001b[38;5;241m*\u001b[39mdtypes_orig)\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[1;32m    882\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "#with RandomizedSearchCV Implementation\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Define features and target\n",
    "features_cl = rolling_stats.select_dtypes(\"number\").drop(columns=[\"FTHG\", \"FTAG\", \"FTR_num\"])\n",
    "target_cl = rolling_stats[\"FTR_num\"]\n",
    "\n",
    "# Step 2: Train a preliminary model to get feature importances\n",
    "preliminary_rf = RandomForestClassifier(random_state=42)\n",
    "preliminary_rf.fit(features_cl, target_cl)\n",
    "\n",
    "# Step 3: Filter features based on importance threshold\n",
    "importance_threshold = 0.1\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': features_cl.columns,\n",
    "    'Importance': preliminary_rf.feature_importances_\n",
    "})\n",
    "important_features = feature_importances[feature_importances['Importance'] >= importance_threshold]['Feature']\n",
    "features_important = features_cl[important_features]\n",
    "\n",
    "# Step 4: Remove zero-variance columns\n",
    "features_important = features_important.loc[:, features_important.var() > 0]\n",
    "\n",
    "# Step 5: Split and scale the filtered dataset\n",
    "X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(features_important, target_cl, random_state=42)\n",
    "\n",
    "# Scaling based on filtered features only\n",
    "scaler = MinMaxScaler()\n",
    "X_train_cl_scaled = scaler.fit_transform(X_train_cl)\n",
    "X_test_cl_scaled = scaler.transform(X_test_cl)\n",
    "\n",
    "# Define parameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [750, 850, 1050],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize RandomForest model and RandomizedSearchCV\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,                    # Number of random combinations to try\n",
    "    cv=5,                         # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,                    # Use all processors\n",
    "    random_state=42               # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV on the scaled training data\n",
    "random_search.fit(X_train_cl_scaled, y_train_cl)\n",
    "\n",
    "# Output the best parameters and the best cross-validation score\n",
    "print(\"Best Parameters from RandomizedSearchCV:\", random_search.best_params_)\n",
    "print(\"Best Cross-Validation Score from RandomizedSearchCV:\", random_search.best_score_)\n",
    "\n",
    "# Evaluation using the best model from RandomizedSearchCV\n",
    "best_rf_model_random = random_search.best_estimator_\n",
    "y_pred_random = best_rf_model_random.predict(X_test_cl_scaled)\n",
    "print(\"\\nRandomizedSearchCV Model Evaluation\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_cl, y_pred_random))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_cl, y_pred_random))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_cl, y_pred_random))\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(final_feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9fb9c0-9c61-4396-b3be-adc763c7c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pickle file for streamlit\n",
    "\n",
    "with open('rf_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_rf_model_random, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8b12d122-d40a-4c28-bdee-cb4f167c4c7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 59\u001b[0m\n\u001b[1;32m     49\u001b[0m halving_search \u001b[38;5;241m=\u001b[39m HalvingGridSearchCV(\n\u001b[1;32m     50\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mrf_model,\n\u001b[1;32m     51\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m                     \u001b[38;5;66;03m# Use all processors\u001b[39;00m\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Fit HalvingGridSearchCV on the scaled training data\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m halving_search\u001b[38;5;241m.\u001b[39mfit(X_train_cl_scaled, y_train_cl)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Output the best parameters and the best cross-validation score\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters from HalvingGridSearchCV:\u001b[39m\u001b[38;5;124m\"\u001b[39m, halving_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search_successive_halving.py:251\u001b[0m, in \u001b[0;36mBaseSuccessiveHalving.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input_parameters(\n\u001b[1;32m    246\u001b[0m     X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, split_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit\n\u001b[1;32m    247\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_samples_orig \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y\u001b[38;5;241m=\u001b[39my, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_score_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv_results_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_test_score\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_index_]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search_successive_halving.py:355\u001b[0m, in \u001b[0;36mBaseSuccessiveHalving._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    348\u001b[0m     cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checked_cv_orig\n\u001b[1;32m    350\u001b[0m more_results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter\u001b[39m\u001b[38;5;124m\"\u001b[39m: [itr] \u001b[38;5;241m*\u001b[39m n_candidates,\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_resources\u001b[39m\u001b[38;5;124m\"\u001b[39m: [n_resources] \u001b[38;5;241m*\u001b[39m n_candidates,\n\u001b[1;32m    353\u001b[0m }\n\u001b[0;32m--> 355\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluate_candidates(\n\u001b[1;32m    356\u001b[0m     candidate_params, cv, more_results\u001b[38;5;241m=\u001b[39mmore_results\n\u001b[1;32m    357\u001b[0m )\n\u001b[1;32m    359\u001b[0m n_candidates_to_keep \u001b[38;5;241m=\u001b[39m ceil(n_candidates \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactor)\n\u001b[1;32m    360\u001b[0m candidate_params \u001b[38;5;241m=\u001b[39m _top_k(results, n_candidates_to_keep, itr)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    918\u001b[0m         clone(base_estimator),\n\u001b[1;32m    919\u001b[0m         X,\n\u001b[1;32m    920\u001b[0m         y,\n\u001b[1;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    927\u001b[0m     )\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[1;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[1;32m    931\u001b[0m     )\n\u001b[1;32m    932\u001b[0m )\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# HalvingGridSearchCV\n",
    "# also takes very long time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define features and target\n",
    "features_cl = rolling_stats.select_dtypes(\"number\").drop(columns=[\"FTHG\", \"FTAG\", \"FTR_num\"])\n",
    "target_cl = rolling_stats[\"FTR_num\"]\n",
    "\n",
    "# Step 2: Train a preliminary model to get feature importances\n",
    "preliminary_rf = RandomForestClassifier(random_state=42)\n",
    "preliminary_rf.fit(features_cl, target_cl)\n",
    "\n",
    "# Step 3: Filter features based on importance threshold\n",
    "importance_threshold = 0.0  # Using 0 threshold as requested\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': features_cl.columns,\n",
    "    'Importance': preliminary_rf.feature_importances_\n",
    "})\n",
    "important_features = feature_importances[feature_importances['Importance'] >= importance_threshold]['Feature']\n",
    "features_important = features_cl[important_features]\n",
    "\n",
    "# Step 4: Remove zero-variance columns\n",
    "features_important = features_important.loc[:, features_important.var() > 0]\n",
    "\n",
    "# Step 5: Split and scale the filtered dataset\n",
    "X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(features_important, target_cl, random_state=42)\n",
    "\n",
    "# Scaling based on filtered features only\n",
    "scaler = MinMaxScaler()\n",
    "X_train_cl_scaled = scaler.fit_transform(X_train_cl)\n",
    "X_test_cl_scaled = scaler.transform(X_test_cl)\n",
    "\n",
    "# Define parameter grid for HalvingGridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 750, 1000, 1250],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize RandomForest model and HalvingGridSearchCV\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "halving_search = HalvingGridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    factor=2,                     # Each iteration reduces the number of candidates by half\n",
    "    scoring='accuracy',\n",
    "    cv=5,                         # 5-fold cross-validation\n",
    "    n_jobs=-1                     # Use all processors\n",
    ")\n",
    "\n",
    "# Fit HalvingGridSearchCV on the scaled training data\n",
    "halving_search.fit(X_train_cl_scaled, y_train_cl)\n",
    "\n",
    "# Output the best parameters and the best cross-validation score\n",
    "print(\"Best Parameters from HalvingGridSearchCV:\", halving_search.best_params_)\n",
    "print(\"Best Cross-Validation Score from HalvingGridSearchCV:\", halving_search.best_score_)\n",
    "\n",
    "# Evaluation using the best model from HalvingGridSearchCV\n",
    "best_rf_model_halving = halving_search.best_estimator_\n",
    "y_pred_halving = best_rf_model_halving.predict(X_test_cl_scaled)\n",
    "print(\"\\nHalvingGridSearchCV Model Evaluation\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_cl, y_pred_halving))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_cl, y_pred_halving))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_cl, y_pred_halving))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf23c0e-ce29-4dc2-bac1-fcbd0b553191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55ac52-df36-4536-8364-e73161de152c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

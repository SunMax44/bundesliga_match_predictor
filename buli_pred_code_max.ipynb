{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 35,
=======
<<<<<<< Updated upstream
   "execution_count": 3,
=======
   "execution_count": 1,
>>>>>>> Stashed changes
   "id": "ca3b2bb6-5cd0-4929-a969-d8d755b2b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import numpy as np\n",
<<<<<<< Updated upstream
    "from sklearn.model_selection import train_test_split\n",
=======
>>>>>>> Stashed changes
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
<<<<<<< Updated upstream
    "from sklearn.tree import DecisionTreeRegressor\n",
=======
    "from sklearn.tree import DecisionTreeRegresso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
>>>>>>> Stashed changes
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
<<<<<<< Updated upstream
=======
    "import pickle\n",
>>>>>>> Stashed changes
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 1,
=======
   "execution_count": 2,
>>>>>>> Stashed changes
>>>>>>> Stashed changes
   "id": "b2089bf0-26cd-4845-9407-dafb1652278c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final combined DataFrame with parsed dates:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "/var/folders/72/b7zxktp96cz1n4tjk3mlbj5w0000gn/T/ipykernel_11984/4226996146.py:34: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
=======
<<<<<<< Updated upstream
      "/var/folders/72/b7zxktp96cz1n4tjk3mlbj5w0000gn/T/ipykernel_2023/1443122606.py:33: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
=======
      "/var/folders/72/b7zxktp96cz1n4tjk3mlbj5w0000gn/T/ipykernel_2264/1889391515.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
>>>>>>> Stashed changes
>>>>>>> Stashed changes
      "  buli_df['Date'] = pd.to_datetime(buli_df['Date'], dayfirst=True, errors='coerce')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTHG</th>\n",
       "      <th>HTAG</th>\n",
       "      <th>HTR</th>\n",
       "      <th>...</th>\n",
       "      <th>B365CAHA</th>\n",
       "      <th>PCAHH</th>\n",
       "      <th>PCAHA</th>\n",
       "      <th>MaxCAHH</th>\n",
       "      <th>MaxCAHA</th>\n",
       "      <th>AvgCAHH</th>\n",
       "      <th>AvgCAHA</th>\n",
       "      <th>Unnamed: 70</th>\n",
       "      <th>Unnamed: 71</th>\n",
       "      <th>Unnamed: 72</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>2015-08-14</td>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D1</td>\n",
       "      <td>2015-08-15</td>\n",
       "      <td>Augsburg</td>\n",
       "      <td>Hertha</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1</td>\n",
       "      <td>2015-08-15</td>\n",
       "      <td>Darmstadt</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D1</td>\n",
       "      <td>2015-08-15</td>\n",
       "      <td>Dortmund</td>\n",
       "      <td>M'gladbach</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D1</td>\n",
       "      <td>2015-08-15</td>\n",
       "      <td>Leverkusen</td>\n",
       "      <td>Hoffenheim</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5503</th>\n",
       "      <td>D1</td>\n",
       "      <td>2013-05-18</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Leverkusen</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504</th>\n",
       "      <td>D1</td>\n",
       "      <td>2013-05-18</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>Fortuna Dusseldorf</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5505</th>\n",
       "      <td>D1</td>\n",
       "      <td>2013-05-18</td>\n",
       "      <td>M'gladbach</td>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5506</th>\n",
       "      <td>D1</td>\n",
       "      <td>2013-05-18</td>\n",
       "      <td>Nurnberg</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5507</th>\n",
       "      <td>D1</td>\n",
       "      <td>2013-05-18</td>\n",
       "      <td>Stuttgart</td>\n",
       "      <td>Mainz</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5508 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Div       Date       HomeTeam            AwayTeam  FTHG  FTAG FTR  HTHG  \\\n",
       "0     D1 2015-08-14  Bayern Munich             Hamburg     5     0   H     1   \n",
       "1     D1 2015-08-15       Augsburg              Hertha     0     1   A     0   \n",
       "2     D1 2015-08-15      Darmstadt            Hannover     2     2   D     1   \n",
       "3     D1 2015-08-15       Dortmund          M'gladbach     4     0   H     3   \n",
       "4     D1 2015-08-15     Leverkusen          Hoffenheim     2     1   H     1   \n",
       "...   ..        ...            ...                 ...   ...   ...  ..   ...   \n",
       "5503  D1 2013-05-18        Hamburg          Leverkusen     0     1   A     0   \n",
       "5504  D1 2013-05-18       Hannover  Fortuna Dusseldorf     3     0   H     1   \n",
       "5505  D1 2013-05-18     M'gladbach       Bayern Munich     3     4   A     3   \n",
       "5506  D1 2013-05-18       Nurnberg       Werder Bremen     3     2   H     0   \n",
       "5507  D1 2013-05-18      Stuttgart               Mainz     2     2   D     2   \n",
       "\n",
       "      HTAG HTR  ...  B365CAHA  PCAHH  PCAHA  MaxCAHH  MaxCAHA  AvgCAHH  \\\n",
       "0        0   H  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "1        0   D  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "2        0   H  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "3        0   H  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "4        1   D  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "...    ...  ..  ...       ...    ...    ...      ...      ...      ...   \n",
       "5503     0   D  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "5504     0   H  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "5505     2   H  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "5506     1   A  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "5507     2   D  ...       NaN    NaN    NaN      NaN      NaN      NaN   \n",
       "\n",
       "      AvgCAHA  Unnamed: 70  Unnamed: 71  Unnamed: 72  \n",
       "0         NaN          NaN          NaN          NaN  \n",
       "1         NaN          NaN          NaN          NaN  \n",
       "2         NaN          NaN          NaN          NaN  \n",
       "3         NaN          NaN          NaN          NaN  \n",
       "4         NaN          NaN          NaN          NaN  \n",
       "...       ...          ...          ...          ...  \n",
       "5503      NaN          NaN          NaN          NaN  \n",
       "5504      NaN          NaN          NaN          NaN  \n",
       "5505      NaN          NaN          NaN          NaN  \n",
       "5506      NaN          NaN          NaN          NaN  \n",
       "5507      NaN          NaN          NaN          NaN  \n",
       "\n",
       "[5508 rows x 141 columns]"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 1,
=======
<<<<<<< Updated upstream
     "execution_count": 3,
=======
     "execution_count": 2,
>>>>>>> Stashed changes
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
=======
>>>>>>> Stashed changes
    "# Specify the folder path where CSV files are stored\n",
    "folder_path = 'data/'\n",
    "\n",
    "# Use glob to find all CSV files in the specified folder\n",
    "all_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# Use a list comprehension to read each CSV file into a DataFrame and ensure 'Date' is string\n",
    "df_list = []\n",
    "for file in all_files:\n",
    "    try:\n",
    "        # Read each CSV and convert 'Date' to string format\n",
    "        buli_df = pd.read_csv(file, encoding='ISO-8859-1', dtype={'Date': str})\n",
    "        df_list.append(buli_df)\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"ParserError parsing {file}: {e}\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"UnicodeDecodeError in {file}: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "buli_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Standardize and parse the 'Date' column\n",
    "buli_df['Date'] = buli_df['Date'].str.strip()  # Remove extra whitespace\n",
    "buli_df['Date'] = buli_df['Date'].replace(r'[/-]', '-', regex=True)  # Replace separators with '-'\n",
    "\n",
    "# Attempt to parse dates as `dayfirst` and handle both `dd/mm/yyyy` and `dd/mm/yy`\n",
    "buli_df['Date'] = buli_df['Date'].apply(lambda x: re.sub(r'(\\d{2}/\\d{2}/)(\\d{2})$', r'\\120\\2', x))\n",
    "buli_df['Date'] = pd.to_datetime(buli_df['Date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# Check for any remaining NaT values in 'Date' after parsing\n",
    "missing_dates = buli_df[buli_df['Date'].isna()]\n",
    "if not missing_dates.empty:\n",
    "    print(\"Warning: Some dates could not be parsed after concatenation.\")\n",
    "    print(missing_dates)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Final combined DataFrame with parsed dates:\")\n",
    "buli_df"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 2,
=======
<<<<<<< Updated upstream
   "execution_count": 5,
=======
   "execution_count": 3,
>>>>>>> Stashed changes
>>>>>>> Stashed changes
   "id": "c54c0ca8-27e2-426e-a160-6ffd4ce0815e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTHG</th>\n",
       "      <th>HTAG</th>\n",
       "      <th>HTR</th>\n",
       "      <th>...</th>\n",
       "      <th>B365CAHA</th>\n",
       "      <th>PCAHH</th>\n",
       "      <th>PCAHA</th>\n",
       "      <th>MaxCAHH</th>\n",
       "      <th>MaxCAHA</th>\n",
       "      <th>AvgCAHH</th>\n",
       "      <th>AvgCAHA</th>\n",
       "      <th>Unnamed: 70</th>\n",
       "      <th>Unnamed: 71</th>\n",
       "      <th>Unnamed: 72</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>2006-08-11</td>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>Dortmund</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D1</td>\n",
       "      <td>2006-08-12</td>\n",
       "      <td>Leverkusen</td>\n",
       "      <td>Aachen</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1</td>\n",
       "      <td>2006-08-12</td>\n",
       "      <td>Mainz</td>\n",
       "      <td>Bochum</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D1</td>\n",
       "      <td>2006-08-12</td>\n",
       "      <td>M'gladbach</td>\n",
       "      <td>Cottbus</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D1</td>\n",
       "      <td>2006-08-12</td>\n",
       "      <td>Schalke 04</td>\n",
       "      <td>Ein Frankfurt</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5503</th>\n",
       "      <td>D1</td>\n",
       "      <td>2024-05-18</td>\n",
       "      <td>Heidenheim</td>\n",
       "      <td>FC Koln</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.86</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.84</td>\n",
       "      <td>2.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504</th>\n",
       "      <td>D1</td>\n",
       "      <td>2024-05-18</td>\n",
       "      <td>Ein Frankfurt</td>\n",
       "      <td>RB Leipzig</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.93</td>\n",
       "      <td>1.96</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5505</th>\n",
       "      <td>D1</td>\n",
       "      <td>2024-05-18</td>\n",
       "      <td>Dortmund</td>\n",
       "      <td>Darmstadt</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5506</th>\n",
       "      <td>D1</td>\n",
       "      <td>2024-05-18</td>\n",
       "      <td>Hoffenheim</td>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5507</th>\n",
       "      <td>D1</td>\n",
       "      <td>2024-05-18</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>Bochum</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>...</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5508 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Div       Date       HomeTeam       AwayTeam  FTHG  FTAG FTR  HTHG  HTAG  \\\n",
       "0     D1 2006-08-11  Bayern Munich       Dortmund     2     0   H     1     0   \n",
       "1     D1 2006-08-12     Leverkusen         Aachen     3     0   H     2     0   \n",
       "2     D1 2006-08-12          Mainz         Bochum     2     1   H     1     0   \n",
       "3     D1 2006-08-12     M'gladbach        Cottbus     2     0   H     0     0   \n",
       "4     D1 2006-08-12     Schalke 04  Ein Frankfurt     1     1   D     1     0   \n",
       "...   ..        ...            ...            ...   ...   ...  ..   ...   ...   \n",
       "5503  D1 2024-05-18     Heidenheim        FC Koln     4     1   H     3     0   \n",
       "5504  D1 2024-05-18  Ein Frankfurt     RB Leipzig     2     2   D     0     1   \n",
       "5505  D1 2024-05-18       Dortmund      Darmstadt     4     0   H     2     0   \n",
       "5506  D1 2024-05-18     Hoffenheim  Bayern Munich     4     2   H     1     2   \n",
       "5507  D1 2024-05-18  Werder Bremen         Bochum     4     1   H     1     0   \n",
       "\n",
       "     HTR  ...  B365CAHA  PCAHH  PCAHA  MaxCAHH  MaxCAHA  AvgCAHH  AvgCAHA  \\\n",
       "0      H  ...       NaN    NaN    NaN      NaN      NaN      NaN      NaN   \n",
       "1      H  ...       NaN    NaN    NaN      NaN      NaN      NaN      NaN   \n",
       "2      H  ...       NaN    NaN    NaN      NaN      NaN      NaN      NaN   \n",
       "3      D  ...       NaN    NaN    NaN      NaN      NaN      NaN      NaN   \n",
       "4      H  ...       NaN    NaN    NaN      NaN      NaN      NaN      NaN   \n",
       "...   ..  ...       ...    ...    ...      ...      ...      ...      ...   \n",
       "5503   H  ...      2.08   1.86   2.07     1.95     2.09     1.84     2.01   \n",
       "5504   A  ...      2.00   1.93   1.96     2.40     2.00     2.05     1.84   \n",
       "5505   H  ...      2.00   1.87   2.02     1.88     2.11     1.85     2.01   \n",
       "5506   A  ...      1.90   2.01   1.90     2.04     1.94     1.95     1.90   \n",
       "5507   H  ...      2.01   1.90   2.00     1.93     2.04     1.86     1.99   \n",
       "\n",
       "      Unnamed: 70  Unnamed: 71  Unnamed: 72  \n",
       "0             NaN          NaN          NaN  \n",
       "1             NaN          NaN          NaN  \n",
       "2             NaN          NaN          NaN  \n",
       "3             NaN          NaN          NaN  \n",
       "4             NaN          NaN          NaN  \n",
       "...           ...          ...          ...  \n",
       "5503          NaN          NaN          NaN  \n",
       "5504          NaN          NaN          NaN  \n",
       "5505          NaN          NaN          NaN  \n",
       "5506          NaN          NaN          NaN  \n",
       "5507          NaN          NaN          NaN  \n",
       "\n",
       "[5508 rows x 141 columns]"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 2,
=======
<<<<<<< Updated upstream
     "execution_count": 5,
=======
     "execution_count": 3,
>>>>>>> Stashed changes
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#formatting the date column to datetime format and sorting by date\n",
    "#buli_df['Date'] = pd.to_datetime(buli_df['Date'])\n",
    "\n",
    "buli_df = buli_df.sort_values(['Date']).reset_index(drop=True)\n",
    "buli_df"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 3,
=======
<<<<<<< Updated upstream
   "execution_count": 7,
=======
   "execution_count": 4,
>>>>>>> Stashed changes
>>>>>>> Stashed changes
   "id": "3441895d-8e09-49f9-9796-1fc0f2f4d09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Div               0\n",
       "Date              0\n",
       "HomeTeam          0\n",
       "AwayTeam          0\n",
       "FTHG              0\n",
       "               ... \n",
       "AvgCAHH        3978\n",
       "AvgCAHA        3978\n",
       "Unnamed: 70    5508\n",
       "Unnamed: 71    5508\n",
       "Unnamed: 72    5508\n",
       "Length: 141, dtype: int64"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 3,
=======
<<<<<<< Updated upstream
     "execution_count": 7,
=======
     "execution_count": 4,
>>>>>>> Stashed changes
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for null values\n",
    "buli_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 4,
=======
<<<<<<< Updated upstream
   "execution_count": 9,
=======
   "execution_count": 5,
>>>>>>> Stashed changes
>>>>>>> Stashed changes
   "id": "0afed34d-9f79-4edc-817d-c5bd06c15893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping rows & columns with all null values\n",
    "buli_df.dropna(axis=1, how='all', inplace=True) #dropped 3 columns\n",
    "buli_df.dropna(axis=0, how='all',inplace=True) #0 rows dropped"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 5,
=======
<<<<<<< Updated upstream
   "execution_count": 11,
=======
   "execution_count": 6,
>>>>>>> Stashed changes
>>>>>>> Stashed changes
   "id": "11a34f86-102c-4261-b4ce-3a960a0ca228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Div            0\n",
       "Date           0\n",
       "HomeTeam       0\n",
       "AwayTeam       0\n",
       "FTHG           0\n",
       "            ... \n",
       "PCAHA       3978\n",
       "MaxCAHH     3978\n",
       "MaxCAHA     3978\n",
       "AvgCAHH     3978\n",
       "AvgCAHA     3978\n",
       "Length: 138, dtype: int64"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 5,
=======
<<<<<<< Updated upstream
     "execution_count": 11,
=======
     "execution_count": 6,
>>>>>>> Stashed changes
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buli_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9588dee0-a2f4-4c19-9b1b-f754829408c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame with selected initial columns and past 7 games stats:\n"
     ]
    }
   ],
   "source": [
    "#code for including all games (first games of the season take the last games of the last season as past games)\n",
    "#4 past games\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "buli_df_red = buli_df[['Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY', 'HR', 'AR']]\n",
    "\n",
    "df = buli_df_red\n",
    "\n",
    "# Define stats dictionary with the specified columns\n",
    "stats = {\n",
    "    'goals': {'scored': ('FTHG', 'FTAG'), 'conceded': ('FTAG', 'FTHG')},\n",
    "    'shots': {'taken': ('HS', 'AS'), 'conceded': ('AS', 'HS')},\n",
    "    'shots_on_target': {'taken': ('HST', 'AST'), 'conceded': ('AST', 'HST')},\n",
    "    'fouls': {'fouls': ('HF', 'AF'), 'fouled': ('AF', 'HF')},\n",
    "    'corners': {'taken': ('HC', 'AC'), 'conceded': ('AC', 'HC')},\n",
    "    'yellow_cards': {'received': ('HY', 'AY'), 'provoked': ('AY', 'HY')},\n",
    "    'red_cards': {'received': ('HR', 'AR'), 'provoked': ('AR', 'HR')},\n",
    "}\n",
    "\n",
    "# Define `npm` for the number of past matches to consider\n",
<<<<<<< Updated upstream
    "npm = 4\n",
=======
<<<<<<< Updated upstream
    "npm = 7\n",
=======
    "npm = 6\n",
>>>>>>> Stashed changes
>>>>>>> Stashed changes
    "\n",
    "# Initialize an empty list to accumulate each row's data as a dictionary\n",
    "rows_list = []\n",
    "\n",
    "# Iterate through each row to calculate rolling stats based on home and away perspectives\n",
    "for index, row in df.iterrows():\n",
    "    team_h = row['HomeTeam']\n",
    "    team_a = row['AwayTeam']\n",
    "    date = row['Date']\n",
    "    \n",
    "    # Get the past `npm` games for the home team, filtered by games before the current match date\n",
    "    past_matches_home = df[((df['HomeTeam'] == team_h) | (df['AwayTeam'] == team_h)) & (df['Date'] < date)]\n",
    "    past_matches_home = past_matches_home.tail(npm)\n",
    "\n",
    "    # Get the past `npm` games for the away team, filtered by games before the current match date\n",
    "    past_matches_away = df[((df['HomeTeam'] == team_a) | (df['AwayTeam'] == team_a)) & (df['Date'] < date)]\n",
    "    past_matches_away = past_matches_away.tail(npm)\n",
    "\n",
    "    # Initialize a dictionary to store the calculated stats for each row\n",
    "    row_stats = {\n",
    "        'Date': date,\n",
    "        'HomeTeam': team_h,\n",
    "        'AwayTeam': team_a,\n",
    "        'FTR': row['FTR'],\n",
    "        'FTHG': row['FTHG'],\n",
    "        'FTAG': row['FTAG'],\n",
    "    }\n",
    "    \n",
    "    # Calculate stats for the home team based on whether they played home or away in past matches\n",
    "    for stat, subcategories in stats.items():\n",
    "        for subcategory, columns in subcategories.items():\n",
    "            home_column, away_column = columns\n",
    "            # Sum the stat when the home team was actually playing at home\n",
    "            stat_home_as_home = past_matches_home.loc[past_matches_home['HomeTeam'] == team_h, home_column].sum()\n",
    "            # Sum the stat when the home team was actually playing as the away team\n",
    "            stat_home_as_away = past_matches_home.loc[past_matches_home['AwayTeam'] == team_h, away_column].sum()\n",
    "            row_stats[f'p_home_{stat}_{subcategory}_last_{npm}'] = stat_home_as_home + stat_home_as_away\n",
    "            \n",
    "    # Calculate stats for the away team based on whether they played home or away in past matches\n",
    "    for stat, subcategories in stats.items():\n",
    "        for subcategory, columns in subcategories.items():\n",
    "            home_column, away_column = columns\n",
    "            # Sum the stat when the away team was actually playing at home\n",
    "            stat_away_as_home = past_matches_away.loc[past_matches_away['HomeTeam'] == team_a, home_column].sum()\n",
    "            # Sum the stat when the away team was actually playing as the away team\n",
    "            stat_away_as_away = past_matches_away.loc[past_matches_away['AwayTeam'] == team_a, away_column].sum()\n",
    "            row_stats[f'p_away_{stat}_{subcategory}_last_{npm}'] = stat_away_as_home + stat_away_as_away\n",
    "\n",
    "    # Calculate points for the home team in the past `npm` games\n",
    "    points_home = (\n",
    "        (past_matches_home.loc[past_matches_home['HomeTeam'] == team_h, 'FTR'] == 'H').sum() * 3 +\n",
    "        (past_matches_home.loc[past_matches_home['AwayTeam'] == team_h, 'FTR'] == 'A').sum() * 3 +\n",
    "        (past_matches_home['FTR'] == 'D').sum() * 1\n",
    "    )\n",
    "    row_stats[f'p_home_points_last_{npm}'] = points_home\n",
    "    \n",
    "    # Calculate points for the away team in the past `npm` games\n",
    "    points_away = (\n",
    "        (past_matches_away.loc[past_matches_away['HomeTeam'] == team_a, 'FTR'] == 'H').sum() * 3 +\n",
    "        (past_matches_away.loc[past_matches_away['AwayTeam'] == team_a, 'FTR'] == 'A').sum() * 3 +\n",
    "        (past_matches_away['FTR'] == 'D').sum() * 1\n",
    "    )\n",
    "    row_stats[f'p_away_points_last_{npm}'] = points_away\n",
    "    \n",
    "    # Append the dictionary for this row to the list\n",
    "    rows_list.append(row_stats)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "rolling_stats = pd.DataFrame(rows_list)\n",
    "\n",
    "# Display the final DataFrame with only the desired columns\n",
    "print(\"Final DataFrame with selected initial columns and past 7 games stats:\")\n",
    "rolling_stats\n",
    "\n",
    "rolling_stats[\"FTR_num\"] = rolling_stats[\"FTR\"].apply(lambda x: 1 if x == \"D\" else (2 if x == \"H\" else 3))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
=======
<<<<<<< Updated upstream
   "execution_count": 191,
   "id": "0e9026b6-dddd-46e0-b160-b013f49c60fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_stats_with_first_games_of_season = rolling_stats"
   ]
  },
  {
   "cell_type": "code",
>>>>>>> Stashed changes
   "execution_count": 193,
   "id": "f2909e2b-a576-4d72-8062-85e4346133ae",
=======
   "execution_count": 10,
   "id": "b155bfc0-ac0d-4c62-9356-287f421dde7a",
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.51\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.36      0.01      0.02       360\n",
      "           2       0.54      0.81      0.64       635\n",
      "           3       0.44      0.47      0.46       382\n",
      "\n",
      "    accuracy                           0.51      1377\n",
      "   macro avg       0.45      0.43      0.37      1377\n",
      "weighted avg       0.46      0.51      0.43      1377\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  4 248 108]\n",
      " [  3 514 118]\n",
      " [  4 198 180]]\n"
     ]
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "#check with csv file if code ran correct\n",
=======
<<<<<<< Updated upstream
    "#columns_to_check = ['p_away_goals_conceded_last_7', 'p_away_goals_scored_last_7', 'p_home_corners_conceived_last_7','p_away_corners_conceived_last_7', 'p_home_yellow_cards_received_last_7','p_home_yellow_cards_provoked_last_7','p_away_corners_taken_last_7']\n",
>>>>>>> Stashed changes
    "rolling_stats_with_first_games_of_season.loc[5461]"
=======
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features_cl = rolling_stats.select_dtypes(\"number\").drop(columns=[\"FTHG\", \"FTAG\", \"FTR_num\"])\n",
    "target_cl = rolling_stats[\"FTR_num\"]\n",
    "X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(features_cl, target_cl, random_state = 0)\n",
    "\n",
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_cl)\n",
    "X_train_cl_scaled = scaler.transform(X_train_cl)\n",
    "X_test_cl_scaled = scaler.transform(X_test_cl)\n",
    "\n",
    "## logistic regression testing the categorical ftr\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "log_reg.fit(X_train_cl_scaled, y_train_cl)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = log_reg.predict(X_test_cl_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_cl, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Displaying classification report and confusion matrix for deeper insights\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_cl, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_cl, y_pred))"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 77,
   "id": "0e9026b6-dddd-46e0-b160-b013f49c60fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "rolling_stats[\"FTR_num\"] = rolling_stats[\"FTR\"].apply(lambda x: 1 if x == \"D\" else (2 if x == \"H\" else 3))\n",
    "\n",
    "features_cl = rolling_stats.select_dtypes(\"number\").drop(columns=[\"FTHG\", \"FTAG\", \"FTR_num\"])\n",
    "target_cl = rolling_stats[\"FTR_num\"]\n",
    "X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(features_cl, target_cl, random_state = 0)\n",
    "\n",
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_cl)\n",
    "X_train_cl_scaled = scaler.transform(X_train_cl)\n",
    "X_test_cl_scaled = scaler.transform(X_test_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b155bfc0-ac0d-4c62-9356-287f421dde7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.01      0.02       358\n",
      "           2       0.52      0.80      0.63       633\n",
      "           3       0.44      0.44      0.44       404\n",
      "\n",
      "    accuracy                           0.49      1395\n",
      "   macro avg       0.42      0.42      0.36      1395\n",
      "weighted avg       0.44      0.49      0.42      1395\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  3 247 108]\n",
      " [  3 509 121]\n",
      " [  4 223 177]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "## logistic regression testing the categorical ftr\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "log_reg.fit(X_train_cl_scaled, y_train_cl)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = log_reg.predict(X_test_cl_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_cl, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Displaying classification report and confusion matrix for deeper insights\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_cl, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_cl, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e1c82f56-a04b-4952-a0ef-612ab052ddbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49032258064516127"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## KNN classifier for categorical FTR\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 140)\n",
    "knn.fit(X_train_cl_scaled, y_train_cl)\n",
    "knn.score(X_test_cl_scaled, y_test_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fb04b911-237a-496b-afb8-a3b12aaf1fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4551971326164875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       358\n",
      "           2       0.46      1.00      0.63       633\n",
      "           3       0.40      0.00      0.01       404\n",
      "\n",
      "    accuracy                           0.46      1395\n",
      "   macro avg       0.29      0.33      0.21      1395\n",
      "weighted avg       0.32      0.46      0.29      1395\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "## Decision Tree CL\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "tree.fit(X_train_cl_scaled, y_train_cl)\n",
    "\n",
    "pred = tree.predict(X_test_cl)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_cl, pred))\n",
    "print(classification_report(y_test_cl, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f25791c9-627e-4e48-b956-6ec5f6cbab37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       358\n",
      "           2       0.49      0.89      0.63       633\n",
      "           3       0.47      0.30      0.37       404\n",
      "\n",
      "    accuracy                           0.49      1395\n",
      "   macro avg       0.32      0.39      0.33      1395\n",
      "weighted avg       0.36      0.49      0.39      1395\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0 297  61]\n",
      " [  0 561  72]\n",
      " [  0 284 120]]\n",
      "\n",
      "Feature Importances:\n",
      "                                   Feature  Importance\n",
      "14              p_away_goals_scored_last_4    0.097611\n",
      "2                p_home_shots_taken_last_4    0.091114\n",
      "17            p_away_shots_conceded_last_4    0.075689\n",
      "28                    p_home_points_last_4    0.075096\n",
      "4      p_home_shots_on_target_taken_last_4    0.069509\n",
      "16               p_away_shots_taken_last_4    0.069465\n",
      "18     p_away_shots_on_target_taken_last_4    0.066421\n",
      "3             p_home_shots_conceded_last_4    0.057082\n",
      "29                    p_away_points_last_4    0.053566\n",
      "0               p_home_goals_scored_last_4    0.050015\n",
      "19  p_away_shots_on_target_conceded_last_4    0.047748\n",
      "8              p_home_corners_taken_last_4    0.033969\n",
      "23          p_away_corners_conceded_last_4    0.033778\n",
      "5   p_home_shots_on_target_conceded_last_4    0.033197\n",
      "22             p_away_corners_taken_last_4    0.022664\n",
      "15            p_away_goals_conceded_last_4    0.021516\n",
      "24     p_away_yellow_cards_received_last_4    0.015598\n",
      "9           p_home_corners_conceded_last_4    0.014969\n",
      "1             p_home_goals_conceded_last_4    0.012473\n",
      "20               p_away_fouls_fouls_last_4    0.011310\n",
      "21              p_away_fouls_fouled_last_4    0.010180\n",
      "6                p_home_fouls_fouls_last_4    0.008925\n",
      "10     p_home_yellow_cards_received_last_4    0.008847\n",
      "7               p_home_fouls_fouled_last_4    0.007701\n",
      "11     p_home_yellow_cards_provoked_last_4    0.003296\n",
      "13        p_home_red_cards_provoked_last_4    0.002658\n",
      "12        p_home_red_cards_received_last_4    0.002089\n",
      "27        p_away_red_cards_provoked_last_4    0.001817\n",
      "25     p_away_yellow_cards_provoked_last_4    0.001698\n",
      "26        p_away_red_cards_received_last_4    0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "## random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(features_cl, target_cl, random_state = 0)\n",
    "\n",
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_cl)\n",
    "X_train_cl_scaled = scaler.transform(X_train_cl)\n",
    "X_test_cl_scaled = scaler.transform(X_test_cl)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "# n_estimators is the number of trees in the forest, and max_depth controls tree depth\n",
    "rf_model = RandomForestClassifier(n_estimators=50, max_depth=4, random_state=42)\n",
    "rf_model.fit(X_train_cl_scaled, y_train_cl)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test_cl_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_cl, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Displaying classification report and confusion matrix for deeper insights\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_cl, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_cl, y_pred))\n",
    "\n",
    "# Optional: Feature importance analysis\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': features_cl.columns,  # Use the columns from X_cl to match the model input\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ed9a8-8438-43fa-96e7-61e57ebf6128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c63a680-7e25-408f-8145-91e60c131d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d53e68d2-a109-4229-b5a0-1f3815d39022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.43\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       371\n",
      "           2       0.44      0.97      0.60       609\n",
      "           3       0.21      0.02      0.03       415\n",
      "\n",
      "    accuracy                           0.43      1395\n",
      "   macro avg       0.21      0.33      0.21      1395\n",
      "weighted avg       0.25      0.43      0.27      1395\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0 361  10]\n",
      " [  0 592  17]\n",
      " [  1 407   7]]\n",
      "\n",
      "Feature Importances:\n",
      "                                   Feature  Importance\n",
      "17            p_away_shots_conceded_last_4    0.046898\n",
      "20               p_away_fouls_fouls_last_4    0.046531\n",
      "21              p_away_fouls_fouled_last_4    0.046236\n",
      "16               p_away_shots_taken_last_4    0.044367\n",
      "3             p_home_shots_conceded_last_4    0.044193\n",
      "5   p_home_shots_on_target_conceded_last_4    0.043887\n",
      "7               p_home_fouls_fouled_last_4    0.043795\n",
      "2                p_home_shots_taken_last_4    0.042687\n",
      "6                p_home_fouls_fouls_last_4    0.042172\n",
      "8              p_home_corners_taken_last_4    0.040022\n",
      "23          p_away_corners_conceded_last_4    0.038487\n",
      "18     p_away_shots_on_target_taken_last_4    0.037773\n",
      "4      p_home_shots_on_target_taken_last_4    0.037458\n",
      "22             p_away_corners_taken_last_4    0.037222\n",
      "19  p_away_shots_on_target_conceded_last_4    0.036442\n",
      "9           p_home_corners_conceded_last_4    0.035947\n",
      "24     p_away_yellow_cards_received_last_4    0.033999\n",
      "14              p_away_goals_scored_last_4    0.033093\n",
      "11     p_home_yellow_cards_provoked_last_4    0.032297\n",
      "15            p_away_goals_conceded_last_4    0.030365\n",
      "0               p_home_goals_scored_last_4    0.030016\n",
      "10     p_home_yellow_cards_received_last_4    0.029924\n",
      "25     p_away_yellow_cards_provoked_last_4    0.029868\n",
      "1             p_home_goals_conceded_last_4    0.026736\n",
      "28                    p_home_points_last_4    0.026428\n",
      "29                    p_away_points_last_4    0.025945\n",
      "26        p_away_red_cards_received_last_4    0.010992\n",
      "27        p_away_red_cards_provoked_last_4    0.009228\n",
      "13        p_home_red_cards_provoked_last_4    0.008677\n",
      "12        p_home_red_cards_received_last_4    0.008316\n"
     ]
    }
   ],
   "source": [
    "## random forest using threwhold filtering\n",
    "\n",
    "# Filter out features below the threshold\n",
    "important_features = feature_importances[feature_importances['Importance'] > importance_threshold]['Feature']\n",
    "features_important = features_cl[important_features]\n",
    "\n",
    "X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(features_important, target_cl, random_state = 42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initialize and train the Random Forest model\n",
    "# n_estimators is the number of trees in the forest, and max_depth controls tree depth\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n",
    "rf_model.fit(X_train_cl_scaled, y_train_cl)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test_cl_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_cl, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Displaying classification report and confusion matrix for deeper insights\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_cl, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_cl, y_pred))\n",
    "\n",
    "# Optional: Feature importance analysis\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': features_cl.columns,  # Use the columns from X_cl to match the model input\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(feature_importances)\n",
    "\n",
    "# Define a threshold for feature importance\n",
    "importance_threshold = 0.01  # Adjust based on importance scores\n",
    "\n",
    "# Filter out features below the threshold\n",
    "important_features = feature_importances[feature_importances['Importance'] > importance_threshold]['Feature']\n",
    "features_important = features_cl[important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf23c0e-ce29-4dc2-bac1-fcbd0b553191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55ac52-df36-4536-8364-e73161de152c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
=======
<<<<<<< Updated upstream
>>>>>>> Stashed changes
   "execution_count": 195,
   "id": "54db633b-02b4-4797-8dd2-d423808d8f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/72/b7zxktp96cz1n4tjk3mlbj5w0000gn/T/ipykernel_1111/2702053440.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Date'] = pd.to_datetime(df['Date'])\n",
      "/var/folders/72/b7zxktp96cz1n4tjk3mlbj5w0000gn/T/ipykernel_1111/2702053440.py:101: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  buli_df_first_games_skipped = buli_df.groupby('Season').apply(lambda x: x.iloc[npm:]).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTR</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>Season</th>\n",
       "      <th>p_home_goals_scored_last_7</th>\n",
       "      <th>p_home_goals_conceded_last_7</th>\n",
       "      <th>p_home_shots_taken_last_7</th>\n",
       "      <th>...</th>\n",
       "      <th>p_away_fouls_fouls_last_7</th>\n",
       "      <th>p_away_fouls_fouled_last_7</th>\n",
       "      <th>p_away_corners_taken_last_7</th>\n",
       "      <th>p_away_corners_conceded_last_7</th>\n",
       "      <th>p_away_yellow_cards_received_last_7</th>\n",
       "      <th>p_away_yellow_cards_provoked_last_7</th>\n",
       "      <th>p_away_red_cards_received_last_7</th>\n",
       "      <th>p_away_red_cards_provoked_last_7</th>\n",
       "      <th>p_home_points_last_7</th>\n",
       "      <th>p_away_points_last_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-08-13</td>\n",
       "      <td>Wolfsburg</td>\n",
       "      <td>Hertha</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-08-13</td>\n",
       "      <td>Hannover</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-08-18</td>\n",
       "      <td>Nurnberg</td>\n",
       "      <td>M'gladbach</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-08-19</td>\n",
       "      <td>Cottbus</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>D</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-08-19</td>\n",
       "      <td>Aachen</td>\n",
       "      <td>Schalke 04</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428</th>\n",
       "      <td>2024-10-26</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>Leverkusen</td>\n",
       "      <td>D</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>59</td>\n",
       "      <td>67</td>\n",
       "      <td>61</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5429</th>\n",
       "      <td>2024-10-26</td>\n",
       "      <td>RB Leipzig</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>58</td>\n",
       "      <td>77</td>\n",
       "      <td>38</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>2024-10-27</td>\n",
       "      <td>Union Berlin</td>\n",
       "      <td>Ein Frankfurt</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>62</td>\n",
       "      <td>32</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5431</th>\n",
       "      <td>2024-10-27</td>\n",
       "      <td>Heidenheim</td>\n",
       "      <td>Hoffenheim</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>74</td>\n",
       "      <td>92</td>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5432</th>\n",
       "      <td>2024-10-27</td>\n",
       "      <td>Bochum</td>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>76</td>\n",
       "      <td>47</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5433 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date       HomeTeam       AwayTeam FTR  FTHG  FTAG  Season  \\\n",
       "0    2006-08-13      Wolfsburg         Hertha   D     0     0       1   \n",
       "1    2006-08-13       Hannover  Werder Bremen   A     2     4       1   \n",
       "2    2006-08-18       Nurnberg     M'gladbach   H     1     0       1   \n",
       "3    2006-08-19        Cottbus        Hamburg   D     2     2       1   \n",
       "4    2006-08-19         Aachen     Schalke 04   A     0     1       1   \n",
       "...         ...            ...            ...  ..   ...   ...     ...   \n",
       "5428 2024-10-26  Werder Bremen     Leverkusen   D     2     2      21   \n",
       "5429 2024-10-26     RB Leipzig       Freiburg   H     3     1      21   \n",
       "5430 2024-10-27   Union Berlin  Ein Frankfurt   D     1     1      21   \n",
       "5431 2024-10-27     Heidenheim     Hoffenheim   D     0     0      21   \n",
       "5432 2024-10-27         Bochum  Bayern Munich   A     0     5      21   \n",
       "\n",
       "      p_home_goals_scored_last_7  p_home_goals_conceded_last_7  \\\n",
       "0                              0                             0   \n",
       "1                              0                             0   \n",
       "2                              3                             0   \n",
       "3                              0                             2   \n",
       "4                              0                             3   \n",
       "...                          ...                           ...   \n",
       "5428                          12                            14   \n",
       "5429                          11                             2   \n",
       "5430                           8                             4   \n",
       "5431                          12                            11   \n",
       "5432                           7                            17   \n",
       "\n",
       "      p_home_shots_taken_last_7  ...  p_away_fouls_fouls_last_7  \\\n",
       "0                             0  ...                          0   \n",
       "1                             0  ...                          0   \n",
       "2                            11  ...                         11   \n",
       "3                            11  ...                         29   \n",
       "4                             8  ...                         17   \n",
       "...                         ...  ...                        ...   \n",
       "5428                         72  ...                         59   \n",
       "5429                         85  ...                         58   \n",
       "5430                         84  ...                         69   \n",
       "5431                         89  ...                         74   \n",
       "5432                         93  ...                         71   \n",
       "\n",
       "      p_away_fouls_fouled_last_7  p_away_corners_taken_last_7  \\\n",
       "0                              0                            0   \n",
       "1                              0                            0   \n",
       "2                             24                            3   \n",
       "3                             22                           11   \n",
       "4                             19                            7   \n",
       "...                          ...                          ...   \n",
       "5428                          67                           61   \n",
       "5429                          77                           38   \n",
       "5430                          62                           32   \n",
       "5431                          92                           32   \n",
       "5432                          76                           47   \n",
       "\n",
       "      p_away_corners_conceded_last_7  p_away_yellow_cards_received_last_7  \\\n",
       "0                                  0                                    0   \n",
       "1                                  0                                    0   \n",
       "2                                  5                                    0   \n",
       "3                                  2                                    3   \n",
       "4                                  5                                    1   \n",
       "...                              ...                                  ...   \n",
       "5428                              22                                   16   \n",
       "5429                              27                                    9   \n",
       "5430                              44                                    9   \n",
       "5431                              42                                   18   \n",
       "5432                              12                                   10   \n",
       "\n",
       "      p_away_yellow_cards_provoked_last_7  p_away_red_cards_received_last_7  \\\n",
       "0                                       0                                 0   \n",
       "1                                       0                                 0   \n",
       "2                                       4                                 0   \n",
       "3                                       2                                 0   \n",
       "4                                       2                                 0   \n",
       "...                                   ...                               ...   \n",
       "5428                                   13                                 0   \n",
       "5429                                   11                                 0   \n",
       "5430                                    9                                 0   \n",
       "5431                                   13                                 1   \n",
       "5432                                    8                                 0   \n",
       "\n",
       "      p_away_red_cards_provoked_last_7  p_home_points_last_7  \\\n",
       "0                                    0                     0   \n",
       "1                                    0                     0   \n",
       "2                                    1                     3   \n",
       "3                                    0                     0   \n",
       "4                                    0                     0   \n",
       "...                                ...                   ...   \n",
       "5428                                 1                    11   \n",
       "5429                                 0                    17   \n",
       "5430                                 0                    14   \n",
       "5431                                 1                     9   \n",
       "5432                                 0                     1   \n",
       "\n",
       "      p_away_points_last_7  \n",
       "0                        0  \n",
       "1                        0  \n",
       "2                        3  \n",
       "3                        1  \n",
       "4                        1  \n",
       "...                    ...  \n",
       "5428                    14  \n",
       "5429                    15  \n",
       "5430                    13  \n",
       "5431                     7  \n",
       "5432                    17  \n",
       "\n",
       "[5433 rows x 37 columns]"
      ]
     },
     "execution_count": 195,
=======
   "execution_count": 11,
   "id": "e1c82f56-a04b-4952-a0ef-612ab052ddbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4989106753812636"
      ]
     },
     "execution_count": 11,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "#code for starting every season only with the 8th games, so that every game of the season that is taken into account has\n",
    "# 7 past games that were played within the very same season\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample data setup (make sure 'Date' column is in datetime format)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Step 1: Identify season breaks by detecting gaps of 2 months or more\n",
    "df = df.sort_values(by='Date').reset_index(drop=True)\n",
    "df['Date_Diff'] = df['Date'].diff().dt.days\n",
    "season_breaks = df[df['Date_Diff'] >= 60].index  # Gaps of 60+ days signify a new season\n",
    "\n",
    "# Step 2: Assign a season identifier\n",
    "df['Season'] = 0\n",
    "current_season = 1\n",
    "for i in range(len(df)):\n",
    "    if i in season_breaks:\n",
    "        current_season += 1\n",
    "    df.at[i, 'Season'] = current_season\n",
    "\n",
    "# Drop the Date_Diff column as it's no longer needed\n",
    "df = df.drop(columns=['Date_Diff'])\n",
    "\n",
    "# Define `npm` for the number of past matches to consider\n",
    "npm = 7\n",
    "\n",
    "# Initialize an empty list to accumulate each row's data as a dictionary\n",
    "rows_list = []\n",
    "\n",
    "# Iterate through each row to calculate rolling stats based on home and away perspectives\n",
    "for index, row in df.iterrows():\n",
    "    team_h = row['HomeTeam']\n",
    "    team_a = row['AwayTeam']\n",
    "    date = row['Date']\n",
    "    \n",
    "    # Get the past `npm` games for the home team within the same season\n",
    "    past_matches_home = df[((df['HomeTeam'] == team_h) | (df['AwayTeam'] == team_h)) & \n",
    "                           (df['Date'] < date) & \n",
    "                           (df['Season'] == row['Season'])]\n",
    "    past_matches_home = past_matches_home.tail(npm)\n",
    "\n",
    "    # Get the past `npm` games for the away team within the same season\n",
    "    past_matches_away = df[((df['HomeTeam'] == team_a) | (df['AwayTeam'] == team_a)) & \n",
    "                           (df['Date'] < date) & \n",
    "                           (df['Season'] == row['Season'])]\n",
    "    past_matches_away = past_matches_away.tail(npm)\n",
    "\n",
    "    # Initialize a dictionary to store the calculated stats for each row\n",
    "    row_stats = {\n",
    "        'Date': date,\n",
    "        'HomeTeam': team_h,\n",
    "        'AwayTeam': team_a,\n",
    "        'FTR': row['FTR'],\n",
    "        'FTHG': row['FTHG'],  # Include Full Time Home Goals directly\n",
    "        'FTAG': row['FTAG'],  # Include Full Time Away Goals directly\n",
    "        'Season': row['Season']\n",
    "    }\n",
    "    \n",
    "    # Calculate stats for the home team based on whether they played home or away in past matches\n",
    "    for stat, subcategories in stats.items():\n",
    "        for subcategory, columns in subcategories.items():\n",
    "            home_column, away_column = columns\n",
    "            # Sum the stat when the home team was actually playing at home\n",
    "            stat_home_as_home = past_matches_home.loc[past_matches_home['HomeTeam'] == team_h, home_column].sum()\n",
    "            # Sum the stat when the home team was actually playing as the away team\n",
    "            stat_home_as_away = past_matches_home.loc[past_matches_home['AwayTeam'] == team_h, away_column].sum()\n",
    "            row_stats[f'p_home_{stat}_{subcategory}_last_{npm}'] = stat_home_as_home + stat_home_as_away\n",
    "            \n",
    "    # Calculate stats for the away team based on whether they played home or away in past matches\n",
    "    for stat, subcategories in stats.items():\n",
    "        for subcategory, columns in subcategories.items():\n",
    "            home_column, away_column = columns\n",
    "            # Sum the stat when the away team was actually playing at home\n",
    "            stat_away_as_home = past_matches_away.loc[past_matches_away['HomeTeam'] == team_a, home_column].sum()\n",
    "            # Sum the stat when the away team was actually playing as the away team\n",
    "            stat_away_as_away = past_matches_away.loc[past_matches_away['AwayTeam'] == team_a, away_column].sum()\n",
    "            row_stats[f'p_away_{stat}_{subcategory}_last_{npm}'] = stat_away_as_home + stat_away_as_away\n",
    "\n",
    "    # Calculate points for the home team in the past `npm` games\n",
    "    points_home = (\n",
    "        (past_matches_home.loc[past_matches_home['HomeTeam'] == team_h, 'FTR'] == 'H').sum() * 3 +\n",
    "        (past_matches_home.loc[past_matches_home['AwayTeam'] == team_h, 'FTR'] == 'A').sum() * 3 +\n",
    "        (past_matches_home['FTR'] == 'D').sum() * 1\n",
    "    )\n",
    "    row_stats[f'p_home_points_last_{npm}'] = points_home\n",
    "    \n",
    "    # Calculate points for the away team in the past `npm` games\n",
    "    points_away = (\n",
    "        (past_matches_away.loc[past_matches_away['HomeTeam'] == team_a, 'FTR'] == 'H').sum() * 3 +\n",
    "        (past_matches_away.loc[past_matches_away['AwayTeam'] == team_a, 'FTR'] == 'A').sum() * 3 +\n",
    "        (past_matches_away['FTR'] == 'D').sum() * 1\n",
    "    )\n",
    "    row_stats[f'p_away_points_last_{npm}'] = points_away\n",
    "    \n",
    "    # Append the dictionary for this row to the list\n",
    "    rows_list.append(row_stats)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "buli_df = pd.DataFrame(rows_list)\n",
    "\n",
    "# Filter out the first `npm` games of each season from the final output\n",
    "buli_df_first_games_skipped = buli_df.groupby('Season').apply(lambda x: x.iloc[npm:]).reset_index(drop=True)\n",
    "\n",
    "# Display the final DataFrame with selected columns and past 7 games stats\n",
    "buli_df_first_games_skipped"
=======
    "## KNN classifier for categorical FTR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features_cl_dt = rolling_stats.select_dtypes(\"number\").drop(columns=[\"FTHG\", \"FTAG\", \"FTR_num\"])\n",
    "target_cl = rolling_stats[\"FTR_num\"]\n",
    "X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(features_cl_dt, target_cl, random_state = 0)\n",
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_cl)\n",
    "X_train_cl_scaled = scaler.transform(X_train_cl)\n",
    "X_test_cl_scaled = scaler.transform(X_test_cl)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 140)\n",
    "knn.fit(X_train_cl_scaled, y_train_cl)\n",
    "knn.score(X_test_cl_scaled, y_test_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb04b911-237a-496b-afb8-a3b12aaf1fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.46114742193173563\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       360\n",
      "           2       0.46      1.00      0.63       635\n",
      "           3       0.00      0.00      0.00       382\n",
      "\n",
      "    accuracy                           0.46      1377\n",
      "   macro avg       0.15      0.33      0.21      1377\n",
      "weighted avg       0.21      0.46      0.29      1377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "## Decision Tree CL\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features_cl_dt = rolling_stats.select_dtypes(\"number\").drop(columns=[\"FTHG\", \"FTAG\", \"FTR_num\"])\n",
    "target_cl = rolling_stats[\"FTR_num\"]\n",
    "X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(features_cl_dt, target_cl, random_state = 0)\n",
    "\n",
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_cl)\n",
    "X_train_cl_scaled = scaler.transform(X_train_cl)\n",
    "X_test_cl_scaled = scaler.transform(X_test_cl)\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "tree.fit(X_train_cl_scaled, y_train_cl)\n",
    "\n",
    "pred = tree.predict(X_test_cl)\n",
    "print(\"Accuracy:\", accuracy_score(y_test_cl, pred))\n",
    "print(classification_report(y_test_cl, pred))"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< Updated upstream
   "id": "fc63f524-9220-4844-819f-f712965b4501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seasons dont fit yet, 2 month definition not really working at some points apparently"
   ]
=======
   "id": "d18ed9a8-8438-43fa-96e7-61e57ebf6128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c63a680-7e25-408f-8145-91e60c131d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d53e68d2-a109-4229-b5a0-1f3815d39022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 750}\n",
      "Best Model Accuracy: 0.47639796659404504\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.39      0.05      0.09       339\n",
      "           2       0.48      0.86      0.62       605\n",
      "           3       0.46      0.28      0.35       433\n",
      "\n",
      "    accuracy                           0.48      1377\n",
      "   macro avg       0.44      0.40      0.35      1377\n",
      "weighted avg       0.45      0.48      0.40      1377\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 17 261  61]\n",
      " [  8 519  78]\n",
      " [ 19 294 120]]\n",
      "\n",
      "Feature Importances:\n",
      "                         Feature  Importance\n",
      "17  p_away_shots_conceded_last_6    0.160429\n",
      "2      p_home_shots_taken_last_6    0.157097\n",
      "3   p_home_shots_conceded_last_6    0.154840\n",
      "16     p_away_shots_taken_last_6    0.153349\n",
      "20     p_away_fouls_fouls_last_6    0.130598\n",
      "6      p_home_fouls_fouls_last_6    0.126598\n",
      "21    p_away_fouls_fouled_last_6    0.117089\n"
     ]
    }
   ],
   "source": [
    "# with Gridsearch cv\n",
    "# takes very very long time\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define features and target\n",
    "features_cl = rolling_stats.select_dtypes(\"number\").drop(columns=[\"FTHG\", \"FTAG\", \"FTR_num\"])\n",
    "target_cl = rolling_stats[\"FTR_num\"]\n",
    "\n",
    "# Step 2: Train a preliminary model to get feature importances\n",
    "preliminary_rf = RandomForestClassifier(random_state=42)\n",
    "preliminary_rf.fit(features_cl, target_cl)\n",
    "\n",
    "# Step 3: Filter features based on importance threshold\n",
    "importance_threshold = 0.04\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': features_cl.columns,\n",
    "    'Importance': preliminary_rf.feature_importances_\n",
    "})\n",
    "important_features = feature_importances[feature_importances['Importance'] > importance_threshold]['Feature']\n",
    "features_important = features_cl[important_features]\n",
    "\n",
    "# Step 4: Remove zero-variance columns from filtered features\n",
    "features_important = features_important.loc[:, features_important.var() > 0]\n",
    "\n",
    "# Step 5: Split and scale the filtered dataset\n",
    "X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(features_important, target_cl, random_state=42)\n",
    "\n",
    "# Scaling based on filtered features only\n",
    "scaler = MinMaxScaler()\n",
    "X_train_cl_scaled = scaler.fit_transform(X_train_cl)\n",
    "X_test_cl_scaled = scaler.transform(X_test_cl)\n",
    "\n",
    "# Step 6: Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 750, 1000, 1250],  # Include values around your optimal 1000\n",
    "    'max_depth': [None, 10, 20, 30],         # Standard depth settings\n",
    "    'min_samples_split': [2, 5, 10],         # Standard split settings\n",
    "    'min_samples_leaf': [1, 2, 4]   \n",
    "}\n",
    "\n",
    "# Initialize RandomForest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Step 7: Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Step 8: Fit GridSearchCV on the training data\n",
    "grid_search.fit(X_train_cl_scaled, y_train_cl)\n",
    "\n",
    "# Step 9: Get the best model from GridSearchCV\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Step 10: Evaluate the best model on the test data\n",
    "y_pred = best_rf_model.predict(X_test_cl_scaled)\n",
    "accuracy = accuracy_score(y_test_cl, y_pred)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"Best Model Accuracy: {accuracy}\")\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_cl, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_cl, y_pred))\n",
    "\n",
    "# Feature importance analysis for the final tuned model\n",
    "final_feature_importances = pd.DataFrame({\n",
    "    'Feature': important_features,  # Now using the selected important features only\n",
    "    'Importance': best_rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(final_feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cbc1e933-7224-49b3-82cd-216ff2189491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from RandomizedSearchCV: {'n_estimators': 750, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': 30}\n",
      "Best Cross-Validation Score from RandomizedSearchCV: 0.508350436684419\n",
      "\n",
      "RandomizedSearchCV Model Evaluation\n",
      "Accuracy: 0.49310094408133626\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.35      0.02      0.03       339\n",
      "           2       0.50      0.87      0.63       605\n",
      "           3       0.48      0.34      0.40       433\n",
      "\n",
      "    accuracy                           0.49      1377\n",
      "   macro avg       0.44      0.41      0.36      1377\n",
      "weighted avg       0.46      0.49      0.41      1377\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  6 248  85]\n",
      " [  4 526  75]\n",
      " [  7 279 147]]\n"
     ]
    }
   ],
   "source": [
    "#with RandomizedSearchCV Implementation\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Define features and target\n",
    "features_cl = rolling_stats.select_dtypes(\"number\").drop(columns=[\"FTHG\", \"FTAG\", \"FTR_num\"])\n",
    "target_cl = rolling_stats[\"FTR_num\"]\n",
    "\n",
    "# Step 2: Train a preliminary model to get feature importances\n",
    "preliminary_rf = RandomForestClassifier(random_state=42)\n",
    "preliminary_rf.fit(features_cl, target_cl)\n",
    "\n",
    "# Step 3: Filter features based on importance threshold\n",
    "importance_threshold = 0.0  # Using 0 threshold as requested\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': features_cl.columns,\n",
    "    'Importance': preliminary_rf.feature_importances_\n",
    "})\n",
    "important_features = feature_importances[feature_importances['Importance'] >= importance_threshold]['Feature']\n",
    "features_important = features_cl[important_features]\n",
    "\n",
    "# Step 4: Remove zero-variance columns\n",
    "features_important = features_important.loc[:, features_important.var() > 0]\n",
    "\n",
    "# Step 5: Split and scale the filtered dataset\n",
    "X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(features_important, target_cl, random_state=42)\n",
    "\n",
    "# Scaling based on filtered features only\n",
    "scaler = MinMaxScaler()\n",
    "X_train_cl_scaled = scaler.fit_transform(X_train_cl)\n",
    "X_test_cl_scaled = scaler.transform(X_test_cl)\n",
    "\n",
    "# Define parameter distribution for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [500, 750, 1000, 1250],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize RandomForest model and RandomizedSearchCV\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,                    # Number of random combinations to try\n",
    "    cv=5,                         # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,                    # Use all processors\n",
    "    random_state=42               # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV on the scaled training data\n",
    "random_search.fit(X_train_cl_scaled, y_train_cl)\n",
    "\n",
    "# Output the best parameters and the best cross-validation score\n",
    "print(\"Best Parameters from RandomizedSearchCV:\", random_search.best_params_)\n",
    "print(\"Best Cross-Validation Score from RandomizedSearchCV:\", random_search.best_score_)\n",
    "\n",
    "# Evaluation using the best model from RandomizedSearchCV\n",
    "best_rf_model_random = random_search.best_estimator_\n",
    "y_pred_random = best_rf_model_random.predict(X_test_cl_scaled)\n",
    "print(\"\\nRandomizedSearchCV Model Evaluation\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_cl, y_pred_random))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_cl, y_pred_random))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_cl, y_pred_random))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9fb9c0-9c61-4396-b3be-adc763c7c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pickle file for streamlit\n",
    "\n",
    "with open('rf_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_rf_model_random, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8b12d122-d40a-4c28-bdee-cb4f167c4c7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 59\u001b[0m\n\u001b[1;32m     49\u001b[0m halving_search \u001b[38;5;241m=\u001b[39m HalvingGridSearchCV(\n\u001b[1;32m     50\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mrf_model,\n\u001b[1;32m     51\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m                     \u001b[38;5;66;03m# Use all processors\u001b[39;00m\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Fit HalvingGridSearchCV on the scaled training data\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m halving_search\u001b[38;5;241m.\u001b[39mfit(X_train_cl_scaled, y_train_cl)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Output the best parameters and the best cross-validation score\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters from HalvingGridSearchCV:\u001b[39m\u001b[38;5;124m\"\u001b[39m, halving_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search_successive_halving.py:251\u001b[0m, in \u001b[0;36mBaseSuccessiveHalving.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input_parameters(\n\u001b[1;32m    246\u001b[0m     X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, split_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit\n\u001b[1;32m    247\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_samples_orig \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y\u001b[38;5;241m=\u001b[39my, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_score_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv_results_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_test_score\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_index_]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search_successive_halving.py:355\u001b[0m, in \u001b[0;36mBaseSuccessiveHalving._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    348\u001b[0m     cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checked_cv_orig\n\u001b[1;32m    350\u001b[0m more_results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter\u001b[39m\u001b[38;5;124m\"\u001b[39m: [itr] \u001b[38;5;241m*\u001b[39m n_candidates,\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_resources\u001b[39m\u001b[38;5;124m\"\u001b[39m: [n_resources] \u001b[38;5;241m*\u001b[39m n_candidates,\n\u001b[1;32m    353\u001b[0m }\n\u001b[0;32m--> 355\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluate_candidates(\n\u001b[1;32m    356\u001b[0m     candidate_params, cv, more_results\u001b[38;5;241m=\u001b[39mmore_results\n\u001b[1;32m    357\u001b[0m )\n\u001b[1;32m    359\u001b[0m n_candidates_to_keep \u001b[38;5;241m=\u001b[39m ceil(n_candidates \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactor)\n\u001b[1;32m    360\u001b[0m candidate_params \u001b[38;5;241m=\u001b[39m _top_k(results, n_candidates_to_keep, itr)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    918\u001b[0m         clone(base_estimator),\n\u001b[1;32m    919\u001b[0m         X,\n\u001b[1;32m    920\u001b[0m         y,\n\u001b[1;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    927\u001b[0m     )\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[1;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[1;32m    931\u001b[0m     )\n\u001b[1;32m    932\u001b[0m )\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# HalvingGridSearchCV\n",
    "# also takes very long time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define features and target\n",
    "features_cl = rolling_stats.select_dtypes(\"number\").drop(columns=[\"FTHG\", \"FTAG\", \"FTR_num\"])\n",
    "target_cl = rolling_stats[\"FTR_num\"]\n",
    "\n",
    "# Step 2: Train a preliminary model to get feature importances\n",
    "preliminary_rf = RandomForestClassifier(random_state=42)\n",
    "preliminary_rf.fit(features_cl, target_cl)\n",
    "\n",
    "# Step 3: Filter features based on importance threshold\n",
    "importance_threshold = 0.0  # Using 0 threshold as requested\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': features_cl.columns,\n",
    "    'Importance': preliminary_rf.feature_importances_\n",
    "})\n",
    "important_features = feature_importances[feature_importances['Importance'] >= importance_threshold]['Feature']\n",
    "features_important = features_cl[important_features]\n",
    "\n",
    "# Step 4: Remove zero-variance columns\n",
    "features_important = features_important.loc[:, features_important.var() > 0]\n",
    "\n",
    "# Step 5: Split and scale the filtered dataset\n",
    "X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(features_important, target_cl, random_state=42)\n",
    "\n",
    "# Scaling based on filtered features only\n",
    "scaler = MinMaxScaler()\n",
    "X_train_cl_scaled = scaler.fit_transform(X_train_cl)\n",
    "X_test_cl_scaled = scaler.transform(X_test_cl)\n",
    "\n",
    "# Define parameter grid for HalvingGridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 750, 1000, 1250],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize RandomForest model and HalvingGridSearchCV\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "halving_search = HalvingGridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    factor=2,                     # Each iteration reduces the number of candidates by half\n",
    "    scoring='accuracy',\n",
    "    cv=5,                         # 5-fold cross-validation\n",
    "    n_jobs=-1                     # Use all processors\n",
    ")\n",
    "\n",
    "# Fit HalvingGridSearchCV on the scaled training data\n",
    "halving_search.fit(X_train_cl_scaled, y_train_cl)\n",
    "\n",
    "# Output the best parameters and the best cross-validation score\n",
    "print(\"Best Parameters from HalvingGridSearchCV:\", halving_search.best_params_)\n",
    "print(\"Best Cross-Validation Score from HalvingGridSearchCV:\", halving_search.best_score_)\n",
    "\n",
    "# Evaluation using the best model from HalvingGridSearchCV\n",
    "best_rf_model_halving = halving_search.best_estimator_\n",
    "y_pred_halving = best_rf_model_halving.predict(X_test_cl_scaled)\n",
    "print(\"\\nHalvingGridSearchCV Model Evaluation\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_cl, y_pred_halving))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_cl, y_pred_halving))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_cl, y_pred_halving))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf23c0e-ce29-4dc2-bac1-fcbd0b553191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55ac52-df36-4536-8364-e73161de152c",
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> Stashed changes
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
